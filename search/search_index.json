{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to my Homepage","text":"<p> I am a data engineer living in Karlsruhe, Germany. My passions are history, politics, sports and software. On this site, I collect a variety of links and blogs and other stuff that I consider interesting for me and maybe a broader audience.</p>"},{"location":"blog/2023/04/23/async-python/","title":"Async Python","text":"","tags":["async","python"]},{"location":"blog/2023/04/23/async-python/#parallelism-in-python","title":"Parallelism in Python","text":"<p>There exists 3 distinct ways to parallelize code in Python, namely threading, multiprocessing, and async. All 3 methods are based upon a different idea. However, the first 2 are a more indirect way to get parallelism, where the operating system's scheduler is involved, whereas the later introduces a completely new paradigm for programming in python (<code>async/await)</code>. </p> <p>In this article, we will quickly introduce each of the concepts, apply them for 2 code examples and performing benchmarks to compare them. Let us first introduce the 2 scripts that we are going to parallelize. </p> <p>The first example <code>sleep.py</code> introduces a simple script, that executed a function do_work 5 times. The function itself just sleeps for 2 seconds, mocking the waiting to an external resource like a database of an HTTPS response:</p> sleep.py<pre><code>import time\nimport datetime\n\n\ndef do_work(number: int):\n    print(f\"{datetime.datetime.now() - start_time}: Start work for {number=}\")\n    time.sleep(2)\n    print(f\"{datetime.datetime.now() - start_time}: Finished work for {number=}\")\n\n\nstart_time = datetime.datetime.now()\nfor i in range(5):\n    do_work(i)\nprint(f\"Finished program after {datetime.datetime.now() - start_time}\")\n</code></pre> <p>The second example <code>hard_work.py</code> has the same structure, however the Python function do_real_work performs actual work on the CPU and calculates a sum using a for loop.</p> hard_work.py<pre><code>import datetime\n\n\ndef do_real_work(number: int):\n    print(f\"{datetime.datetime.now()-start_time}: Start work for {number=}\")\n    output = 0\n    for i in range(100_000_000):\n        output += 1\n    print(f\"{datetime.datetime.now()-start_time}: Finished work for {number=}\")\n\nstart_time = datetime.datetime.now()\nfor i in range(5):\n    do_real_work(i)\nprint(f\"Finished program after {datetime.datetime.now() - start_time}\")\n</code></pre>","tags":["async","python"]},{"location":"blog/2023/04/23/async-python/#synchronous-execution","title":"Synchronous execution","text":"<p>When we execute the 2 scripts above, we get the following output <pre><code>&gt;&gt;&gt; python3 sleep.py\n\n0:00:00.000010: Start work for number=0\n0:00:02.005169: Finished work for number=0\n0:00:02.005421: Start work for number=1\n0:00:04.008039: Finished work for number=1\n0:00:04.008167: Start work for number=2\n0:00:06.010063: Finished work for number=2\n0:00:06.010195: Start work for number=3\n0:00:08.011329: Finished work for number=3\n0:00:08.011481: Start work for number=4\n0:00:10.016531: Finished work for number=4\nFinished program after 0:00:10.016634\n</code></pre></p> <p>and: <pre><code>&gt;&gt;&gt; python3 hard_work.py\n\n0:00:00.000005: Start work for number=0\n0:00:02.352451: Finished work for number=0\n0:00:02.352548: Start work for number=1\n0:00:04.687044: Finished work for number=1\n0:00:04.687078: Start work for number=2\n0:00:07.084300: Finished work for number=2\n0:00:07.084339: Start work for number=3\n0:00:09.425129: Finished work for number=3\n0:00:09.425161: Start work for number=4\n0:00:11.770652: Finished work for number=4\nFinished program after 0:00:11.770680\n</code></pre></p> <p>Thus we can see the following execution times<sup>1</sup>:</p> Synchronous Execution sleep.py 10 do_work.py 11.8","tags":["async","python"]},{"location":"blog/2023/04/23/async-python/#threading","title":"Threading","text":"<p>Using the <code>threading</code> module in Python, one cas easily execute parallel tasks. We then get for the first example:</p> CodeOutput sleep.py<pre><code>import time\nimport datetime\nfrom threading import Thread\n\n\ndef do_work(number: int):\n    print(f\"{datetime.datetime.now() - start_time}: Start work for {number=}\")\n    time.sleep(2)\n    print(f\"{datetime.datetime.now() - start_time}: Finished work for {number=}\")\n\n\nstart_time = datetime.datetime.now()\n\ntasks = [Thread(target=do_work, args=(i,)) for i in range(5)]\nfor task in tasks:\n    task.start()\nfor task in tasks:\n    task.join()\n\nprint(f\"Finished program after {datetime.datetime.now() - start_time}\")\n</code></pre> <pre><code>&gt;&gt;&gt; python3 sleep.py\n\n0:00:00.000071: Start work for number=0\n0:00:00.000126: Start work for number=1\n0:00:00.000168: Start work for number=2\n0:00:00.000205: Start work for number=3\n0:00:00.000246: Start work for number=4\n0:00:02.005241: Finished work for number=0\n0:00:02.005388: Finished work for number=4\n0:00:02.005453: Finished work for number=1\n0:00:02.005475: Finished work for number=2\n0:00:02.005430: Finished work for number=3\nFinished program after 0:00:02.006002\n</code></pre> <p>As can be seen, the program has finished in only 2 seconds instead of 10 seconds, because alls tasks have been processed in parallel and have a runtime of 2 seconds.</p> <p>However, when we do the same for the other example:</p> CodeOutput hard_work.py<pre><code>import datetime\nfrom threading import Thread\n\ndef do_real_work(number: int):\n    print(f\"{datetime.datetime.now()-start_time}: Start work for {number=}\")\n    output = 0\n    for i in range(100_000_000):\n        output += 1\n    print(f\"{datetime.datetime.now()-start_time}: Finished work for {number=}\")\n\nstart_time = datetime.datetime.now()\n\ntasks = [Thread(target=do_real_work, args=(i,)) for i in range(5)]\nfor task in tasks:\n    task.start()\nfor task in tasks:\n    task.join()\n\nprint(f\"Finished program after {datetime.datetime.now() - start_time}\")\n</code></pre> <pre><code>&gt;&gt;&gt; python3 hard_work.py\n\n0:00:00.000125: Start work for number=0\n0:00:00.000200: Start work for number=1\n0:00:00.019017: Start work for number=2\n0:00:00.049231: Start work for number=3\n0:00:00.080564: Start work for number=4\n0:00:10.813826: Finished work for number=2\n0:00:10.885100: Finished work for number=3\n0:00:10.988489: Finished work for number=0\n0:00:11.014248: Finished work for number=1\n0:00:11.020219: Finished work for number=4\nFinished program after 0:00:11.020369\n</code></pre> <p>we still need 11 seconds like in the synchronous execution. The reason for this lies in the way threading is handled by the CPython interpreter. Due to the Global Interpreter Lock (GIL), Python ensures that at each moment only one thread can be actively executing the code. Therefore, we are executing multiple functions in parallel, but we are effectively still using only 1 core at a time. This can be seen in the image below.</p> <p> </p> Work of a Python code with threads (GIL blocking of true parallelism) <p>In the sleep example above, the function was basically doing nothing except waiting (mimicking for example a wait time due to a database or HTTP request). Therefore, the parallel execution works fine, because there is no real work to be done.</p> <p>For the <code>hard_work.py</code> code snippet, the execution time is really spent doing actual Python code execution. As can be seen in the output, all 5 tasks will start nearly the same time, but since it will only get 1/5 of the CPU, each task will need about 5 times as much time. As a result, there is no speed improvement when using <code>threading in Python</code> for computiationally expensive applications.</p> <p>The observed execution times so far:</p> Synchronous Execution Threading sleep.py 10 2 do_work.py 11.8 11","tags":["async","python"]},{"location":"blog/2023/04/23/async-python/#multiprocessing","title":"Multiprocessing","text":"<p>Real <code>multiprocessing</code> can be used in Python very similar to Threads. Note that in this case, the processed are encapulated in seperate child processed and cannot (easily) access the data from other processes. Let us again modify our two code examples:</p> CodeOutput sleep.py<pre><code>import time\nimport datetime\nfrom multiprocessing import Process\n\n\ndef do_work(number: int):\n    print(f\"{datetime.datetime.now() - start_time}: Start work for {number=}\")\n    time.sleep(2)\n    print(f\"{datetime.datetime.now() - start_time}: Finished work for {number=}\")\n\n\nstart_time = datetime.datetime.now()\n\ntasks = [Process(target=do_work, args=(i,)) for i in range(5)]\nfor task in tasks:\n    task.start()\nfor task in tasks:\n    task.join()\n\nprint(f\"Finished program after {datetime.datetime.now() - start_time}\")\n</code></pre> <pre><code>&gt;&gt;&gt; python3 sleep.py\n\n0:00:00.006796: Start work for number=0\n0:00:00.007449: Start work for number=1\n0:00:00.008078: Start work for number=2\n0:00:00.008823: Start work for number=3\n0:00:00.009255: Start work for number=4\n0:00:02.008293: Finished work for number=0\n0:00:02.008291: Finished work for number=1\n0:00:02.009784: Finished work for number=3\n0:00:02.009597: Finished work for number=2\n0:00:02.010936: Finished work for number=4\nFinished program after 0:00:02.013174\n</code></pre> CodeOutput hard_work.py<pre><code>import datetime\nfrom multiprocessing import Process\n\ndef do_real_work(number: int):\n    print(f\"{datetime.datetime.now()-start_time}: Start work for {number=}\")\n    output = 0\n    for i in range(100_000_000):\n        output += 1\n    print(f\"{datetime.datetime.now()-start_time}: Finished work for {number=}\")\n\nstart_time = datetime.datetime.now()\n\ntasks = [Process(target=do_real_work, args=(i,)) for i in range(5)]\nfor task in tasks:\n    task.start()\nfor task in tasks:\n    task.join()\n\nprint(f\"Finished program after {datetime.datetime.now() - start_time}\")\n</code></pre> <pre><code>&gt;&gt;&gt; python3 hard_work.py\n\n0:00:00.007729: Start work for number=0\n0:00:00.008396: Start work for number=1\n0:00:00.008928: Start work for number=2\n0:00:00.011777: Start work for number=3\n0:00:00.012632: Start work for number=4\n0:00:02.649805: Finished work for number=2\n0:00:02.655102: Finished work for number=1\n0:00:02.688040: Finished work for number=3\n0:00:02.738391: Finished work for number=4\n0:00:02.888424: Finished work for number=0\nFinished program after 0:00:02.889433\n</code></pre> <p>Now, we finally see real Python use several cores of our CPU to execute the tasks in parallel. This leads us to the following execution times:</p> Synchronous Execution Threading Multiprocessing sleep.py 10 2 2 do_work.py 11.8 11 2.8","tags":["async","python"]},{"location":"blog/2023/04/23/async-python/#asynchronous-python","title":"Asynchronous Python","text":"<p>The last paradigm to execute Python code in parallel is the use of <code>async/await</code>. This method kind of resembles the <code>threading</code> idea. However, in this case it is not the OS Scheduler that is assigning resources to the tasks/threads, but the async event loop scheduler of Python<sup>2</sup>. There are several key advantages in comparison to regular threading:</p> <ol> <li>Reduces overhead since no additional threads have to be scheduled by the OS</li> <li>More parallel tasks can be scheduled in parallel</li> <li><code>async/await</code> allows for control, where tasks can be paused and resumed</li> </ol> <p>Let us now have a look at the <code>sleep</code> example:</p> CodeOutput sleep.py<pre><code>import datetime\nimport asyncio\n\n\nasync def do_work(number: int):\n    print(f\"{datetime.datetime.now() - start_time}: Start work for {number=}\")\nawait asyncio.sleep(2)\nprint(f\"{datetime.datetime.now() - start_time}: Finished work for {number=}\")\n\n\nasync def main():\n    tasks = [asyncio.create_task(do_work(i)) for i in range(5)]\n    for task in tasks:\nawait task\nif __name__ == \"__main__\":\n    start_time = datetime.datetime.now()\n    asyncio.run(main())\n    print(f\"Finished program after {datetime.datetime.now() - start_time}\")\n</code></pre> <pre><code>&gt;&gt;&gt; python3 sleep.py\n\n0:00:00.000177: Start work for number=0\n0:00:00.000206: Start work for number=1\n0:00:00.000214: Start work for number=2\n0:00:00.000219: Start work for number=3\n0:00:00.000224: Start work for number=4\n0:00:02.001531: Finished work for number=0\n0:00:02.001620: Finished work for number=1\n0:00:02.001637: Finished work for number=2\n0:00:02.001648: Finished work for number=3\n0:00:02.001660: Finished work for number=4\nFinished program after 0:00:02.002334\n</code></pre> <p>Similar to the <code>threading</code> and <code>multiprocessing</code> case, we could speed up the program by about a factor 5 here. The key differences here is the way we layout our program:</p> <ol> <li>Using the await keyword, we can tell the event loop to pause the execution of the function and work on another task. The event loop will then execute another task, until another await will be called, where the scheduler will give the control to yet another task. In the code above, each part where we tell the event loop that it can temporarily suspend the active task is highlighted. In the real world, these awaited calls often go to systems where we have to wait for a response like Databases, IO or HTTP Calls.</li> <li>In line 19, we define the async event loop, that will be responsible for scheduling the resources between the tasks, and tell him to execute our async `main function.</li> <li>Tasks can be scheduled using <code>asyncio.create_task</code>, which will then be executed in parallel. Calling await on the tasks will wait for the task to be executed (similar to <code>Thread.join</code>). If the async function has a return value, it can just be assigned via <code>value = await task</code>. </li> <li>Only async functions can be awaited. Note, that you can just call regular synchronous code (like print) inside async functions, however it is not possible to call async functions from synchronous code!</li> </ol> <p>Asynchronous Python Code allow for true parallelism without the use of Threads or Processes, however there is still only one execution of Python Code at each time. Therefore, the <code>hard work</code> code:</p> CodeOutput hard_work.py<pre><code>async def do_real_work(number: int):\n    print(f\"{datetime.datetime.now()-start_time}: Start work for {number=}\")\n    output = 0\n    for i in range(100_000_000):\n        output += 1\n    print(f\"{datetime.datetime.now()-start_time}: Finished work for {number=}\")\n\n\nasync def main():\n    tasks = [asyncio.create_task(do_real_work(i)) for i in range(5)]\n    for task in tasks:\n        await task\n\n\nif __name__ == \"__main__\":\n    start_time = datetime.datetime.now()\n    asyncio.run(main())\n    print(f\"Finished program after {datetime.datetime.now() - start_time}\")\n</code></pre> <pre><code>&gt;&gt;&gt; python3 hard_work.py\n\n0:00:00.000176: Start work for number=0\n0:00:02.103602: Finished work for number=0\n0:00:02.103778: Start work for number=1\n0:00:04.209974: Finished work for number=1\n0:00:04.210021: Start work for number=2\n0:00:06.322592: Finished work for number=2\n0:00:06.322637: Start work for number=3\n0:00:08.446054: Finished work for number=3\n0:00:08.446091: Start work for number=4\n0:00:10.562800: Finished work for number=4\nFinished program after 0:00:10.563345\n</code></pre> <p>does not show a big improvement like in the <code>multiprocessing</code> example, leading to the final execution time overview:</p> Synchronous Execution Threading Multiprocessing Async/Await sleep.py 10 2 2 2 do_work.py 11.8 11.02 2.8 10.6","tags":["async","python"]},{"location":"blog/2023/04/23/async-python/#deeper-dive-into-asyncawait","title":"Deeper dive into Async/Await","text":"","tags":["async","python"]},{"location":"blog/2023/04/23/async-python/#parallel-http-requests","title":"Parallel HTTP Requests","text":"<p>The real strength of asynchronous Code can be seen, if you are accessing external systems, where you have to wait for the response and the event loop can use the waiting time to do other things. Let us consider the following example of calling the PokeAPI to list us details about each Pokemon. We are using the awesome httpx libary, which is an async drop-in replacement for <code>requests</code>:</p> CodeOutput unparallel_async.py<pre><code>import asyncio\nimport datetime\n\nimport httpx\n\nasync def main():\n    async with httpx.AsyncClient(base_url=\"\") as client:\n        all_pokemon_response = await client.get(\"https://pokeapi.co/api/v2/pokemon\", params={\"limit\": 10000})\n        all_pokemon_response.raise_for_status()\n        all_pokemon = all_pokemon_response.json()[\"results\"]\n\n        for pokemon in all_pokemon:\n            print(f\"Get information for `{pokemon['name']}`\")\n            pokemon_details_response = await client.get(pokemon[\"url\"])\n            pokemon_details_response.raise_for_status()\n            pokemon_details = pokemon_details_response.json()\n            print(f'ID: {pokemon_details[\"id\"]}, Name: {pokemon_details[\"name\"]}, Height: {pokemon_details[\"height\"]}, Weight: {pokemon_details[\"weight\"]}')\n\nif __name__ == \"__main__\":\n    start_time = datetime.datetime.now()\n    asyncio.run(main())\n    print(f\"Finished program after {datetime.datetime.now() - start_time}\")`\n</code></pre> <pre><code>&gt;&gt;&gt; python3 unparallel_async.py\n\nGet information for `bulbasaur`\nID: 1, Name: bulbasaur, Height: 7, Weight: 69\nGet information for `ivysaur`\nID: 2, Name: ivysaur, Height: 10, Weight: 130\nGet information for `venusaur`\nID: 3, Name: venusaur, Height: 20, Weight: 1000\n\n...\nGet information for `miraidon-glide-mode`\nID: 10271, Name: miraidon-glide-mode, Height: 28, Weight: 2400\nFinished program after 0:03:15.133072\n</code></pre> <p>Downloading the details about ~1000 Pokemon took about 3 minutes for the program. This program seems to be asynchronous, but if you look closely, we are never calling an <code>asyncio.create_task</code> or <code>asyncio.gather</code> function that schedules async tasks in parallel. If you inspect the output you will see that the program just iterates through all Pokemon URLs and each times waits until each single request is finished.</p> <p>With a little change, the program can be refactored to be fully asynchronous:</p> CodeOutput parallel_async.py<pre><code>import asyncio\nimport datetime\n\nimport httpx\n\n\nasync def print_pokemon_details(client: httpx.AsyncClient, pokemon: dict[str, str]):\n    print(f\"Get information for `{pokemon['name']}`\")\n    pokemon_details_response = await client.get(pokemon[\"url\"])\n    pokemon_details_response.raise_for_status()\n    pokemon_details = pokemon_details_response.json()\n    print(\n        f'ID: {pokemon_details[\"id\"]}, Name: {pokemon_details[\"name\"]}, Height: {pokemon_details[\"height\"]}, Weight: {pokemon_details[\"weight\"]}'\n    )\n\n\nasync def main():\n    async with httpx.AsyncClient(base_url=\"\") as client:\n        all_pokemon_response = await client.get(\n            \"https://pokeapi.co/api/v2/pokemon\", params={\"limit\": 10000}\n        )\n        all_pokemon_response.raise_for_status()\n        all_pokemon = all_pokemon_response.json()[\"results\"]\n\n        await asyncio.gather(\n            *[print_pokemon_details(client, pokemon) for pokemon in all_pokemon]\n        )\n\n\nif __name__ == \"__main__\":\n    start_time = datetime.datetime.now()\n    asyncio.run(main())\n    print(f\"Finished program after {datetime.datetime.now() - start_time}\")\n</code></pre> <pre><code>&gt;&gt;&gt; python3 parallel_async.py\n\nGet information for `bulbasaur`\nGet information for `ivysaur`\nGet information for `venusaur`\nGet information for `charmander`\nGet information for `charmeleon`\n...\nGet information for `miraidon-aquatic-mode`\nGet information for `miraidon-glide-mode`\nID: 1, Name: bulbasaur, Height: 7, Weight: 69\nID: 4, Name: charmander, Height: 6, Weight: 85\nID: 3, Name: venusaur, Height: 20, Weight: 1000\n...\nID: 10187, Name: morpeko-hangry, Height: 3, Weight: 30\nID: 10226, Name: urshifu-single-strike-gmax, Height: 290, Weight: 10000\nID: 10131, Name: minior-yellow-meteor, Height: 3, Weight: 400\nFinished program after 0:00:04.860879\n</code></pre> <p>This async version is about 50 times faster than the previous one! As can be seen, all ~1000 functions have first started and then requested and printed the result. The execution flow is shown below</p> <p> </p> Async Execution Flow for Pokemon API Example","tags":["async","python"]},{"location":"blog/2023/04/23/async-python/#building-async-pipeline","title":"Building async pipeline","text":"<p>So far, we used <code>asyncio.gather</code> or <code>asyncio.create_task</code> + <code>wait task</code> to create tasks in parallel. However, for this to work, we have to know which tasks we want to execute in advance. Let us now consider an example, where we have a slow service, that serves us with the URLs to request the Pokemon details:</p> CodeOutput pokemon_pipeline.py<pre><code>import asyncio\nimport datetime\nfrom typing import AsyncIterator\n\nimport httpx\n\n\nasync def get_pokemons(client: httpx.AsyncClient) -&gt; AsyncIterator[dict]:\n    all_pokemon_response = await client.get(\n        \"https://pokeapi.co/api/v2/pokemon\", params={\"limit\": 10000}\n    )\n    all_pokemon_response.raise_for_status()\n    for pokemon in all_pokemon_response.json()[\"results\"]:\n        print(f\"Get Url for Pokemon {pokemon['name']}\")\n        # This is a slow producer, so we have to sleep:\n        await asyncio.sleep(0.01)\n        yield pokemon\n\n\nasync def print_pokemon_details(client: httpx.AsyncClient, pokemon: dict[str, str]):\n    print(f\"Get information for `{pokemon['name']}`\")\n    pokemon_details_response = await client.get(pokemon[\"url\"])\n    pokemon_details_response.raise_for_status()\n    pokemon_details = pokemon_details_response.json()\n    print(\n        f'ID: {pokemon_details[\"id\"]}, Name: {pokemon_details[\"name\"]}, Height: {pokemon_details[\"height\"]}, Weight: {pokemon_details[\"weight\"]}'\n    )\n\n\nasync def main():\n    async with httpx.AsyncClient(base_url=\"\") as client:\n        pokemons = get_pokemons(client)\n        await asyncio.gather(\n            *[print_pokemon_details(client, pokemon) async for pokemon in pokemons]\n        )\n\n\nif __name__ == \"__main__\":\n    start_time = datetime.datetime.now()\n    asyncio.run(main())\n    print(f\"Finished program after {datetime.datetime.now() - start_time}\")\n</code></pre> <pre><code>&gt;&gt;&gt; python3 pokemon_pipeline.py\n\n    Get Url for Pokemon bulbasaur\n    Get Url for Pokemon ivysaur\n    Get Url for Pokemon venusaur\n    ...\n    Get information for `bulbasaur`\n    Get information for `ivysaur`\n    Get information for `venusaur`\n    ...\n    ID: 10084, Name: pikachu-libre, Height: 4, Weight: 60\n    ID: 879, Name: copperajah, Height: 30, Weight: 6500\n    Finished program after 0:00:18.303518\n</code></pre> <p>As can be seen in the output , we first get all Pokemon URLs and after that we start our asynchronous HTTP Calls to get the Pokemon Details. This is not what we want. However, there is a way to already start the downloads of the details when we get our first URLs by using a Queue Mechanism:</p> CodeOutput pokemon_pipeline.py<pre><code>import asyncio\nimport datetime\nfrom typing import AsyncIterator\n\nimport httpx\n\n\nasync def pokemons_producer(\n        client: httpx.AsyncClient, pokemons: asyncio.Queue\n) -&gt; AsyncIterator[dict]:\n    all_pokemon_response = await client.get(\n        \"https://pokeapi.co/api/v2/pokemon\", params={\"limit\": 10000}\n    )\n    all_pokemon_response.raise_for_status()\n    for pokemon in all_pokemon_response.json()[\"results\"]:\n        print(f\"Get Url for Pokemon {pokemon['name']}\")\n        # This is a slow producer, so we have to sleep:\n        await asyncio.sleep(0.01)\n        await pokemons.put(pokemon)\n    await pokemons.put(None)\n\n\nasync def print_pokemon_details(client: httpx.AsyncClient, pokemon: dict):\n    print(f\"Get information for `{pokemon['name']}`\")\n    pokemon_details_response = await client.get(pokemon[\"url\"])\n    pokemon_details_response.raise_for_status()\n    pokemon_details = pokemon_details_response.json()\n    print(\n        f'ID: {pokemon_details[\"id\"]}, Name: {pokemon_details[\"name\"]}, Height: {pokemon_details[\"height\"]}, Weight: {pokemon_details[\"weight\"]}'\n    )\n\n\nasync def pokemon_details_consumer(client: httpx.AsyncClient, pokemons: asyncio.Queue):\n    consumer_active = True\n    while consumer_active:\n        await asyncio.sleep(0.05)\n        pokemons_to_process = []\n        while not pokemons.empty():\n            if (pokemon := await pokemons.get()) is not None:\n                pokemons_to_process.append(pokemon)\n            else:\n                consumer_active = False\n        await asyncio.gather(\n            *(print_pokemon_details(client, pokemon) for pokemon in pokemons_to_process)\n        )\n\n\nasync def main():\n    async with httpx.AsyncClient(base_url=\"\") as client:\n        pokemons = asyncio.Queue()\n        pokemon_producer_task = asyncio.create_task(pokemons_producer(client, pokemons))\n        pokemon_details_consumer_task = asyncio.create_task(\n            pokemon_details_consumer(client, pokemons)\n        )\n        await asyncio.gather(*[pokemon_producer_task, pokemon_details_consumer_task])\n\n\nif __name__ == \"__main__\":\n    start_time = datetime.datetime.now()\n    asyncio.run(main())\n    print(f\"Finished program after {datetime.datetime.now() - start_time}\")\n</code></pre> <pre><code>&gt;&gt;&gt; python3 pokemon_pipeline.py\n\nGet Url for Pokemon bulbasaur\nGet Url for Pokemon ivysaur\nGet Url for Pokemon venusaur\nGet Url for Pokemon charmander\nGet information for `bulbasaur`\nGet information for `ivysaur`\nGet information for `venusaur`\nGet information for `charmander`\nGet Url for Pokemon charmeleon\nGet Url for Pokemon charizard\nID: 1, Name: bulbasaur, Height: 7, Weight: 69\nGet Url for Pokemon squirtle\nGet Url for Pokemon wartortle\nGet Url for Pokemon blastoise\nID: 2, Name: ivysaur, Height: 10, Weight: 130\nID: 4, Name: charmander, Height: 6, Weight: 85\n...\nFinished program after 0:00:15.070663\n</code></pre> <p>Here, we have a producer (generates the Pokemon Details URLs) and a consumer task (exports details from URL). They communicate with each other via an <code>asyncio.Queue</code>, such that both can operate in parallel and we can start extracting the Pokemon Details as soon as the first URL has been published by the produced task. Using the same idea, it is then of course possible to generate a cascade of processes, where each intermediate task consumes from one task and produces objects that will then be processed by the next task.</p> <ol> <li> <p>It is usually better to use the builtin timeit module for the extraction of execution times, however in this example just running the code is sufficient to get an idea of the key differences in performance. The benchmarks have been performed on a Macbook Pro 13 with M1 chip.\u00a0\u21a9</p> </li> <li> <p>Note that you can either use the builtin asyncio event loop or alternative implementations like uvloop.\u00a0\u21a9</p> </li> </ol>","tags":["async","python"]},{"location":"blog/2023/08/01/5-reasons-why-code-comments-are-a-code-smell/","title":"5 Reasons why Code Comments are a Code Smell","text":"","tags":["clean code","comments"]},{"location":"blog/2023/08/01/5-reasons-why-code-comments-are-a-code-smell/#1-comments-that-just-describe-what-is-happening","title":"1. Comments that just describe what is happening","text":"<p>Let's have a look at the following code example: <pre><code>#################\n#### Imports ####\n#################\nimport os.path\n\n# Define path to executable:\npath = \"/usr/bin/executable\"\n\n# Extract directory: \nd = os.path.dirname(path) \n\n# Print Directory of executable:\nprint(d)\n</code></pre></p> <p>These comments just obvioulsy just describe what happens in the code below them. This is however not necessary, because the code is easy to read if we just name the variables better:</p> <pre><code>import os.path\n\npath_to_executable = \"/usr/bin/executable\"\ndirectory_of_executable = os.path.dirname(path_to_executable)\nprint(directory_of_executable)\n</code></pre>","tags":["clean code","comments"]},{"location":"blog/2023/08/01/5-reasons-why-code-comments-are-a-code-smell/#2-comments-are-a-sign-of-too-complex-logic","title":"2. Comments are a sign of too complex logic","text":"<p>Often, we see code comments when the code deals with complex or nested logic:</p> <pre><code># We do have to check, that the credit card of the user is not expired and that the user is solvent.\nif user.state.value == \"solvent\" and user.credit_card.get_expiration_dt &lt; datetime.datetime.now().date:\n    # If we process Visa cards, online payment service has to be enabled:\n    if not (user.credit_card.type == \"Visa\" and \"online_payment\" in user.credit_card.payment_services):\n        raise OnlinePaymentServiceNotEnabledException(\"Visa Payment not possible.\") \n    process_payment(user.credit_card, payment_details)\nelse:\n    raise PaymentException(\"...\")\n</code></pre> <p>This is not optimal to read and also there is the danger, that the comments will be outdated (see below) after some time. It is in this case better to define small functions with a proper name that describe what happens within the code:</p> <pre><code>if is_solvent(user) and credit_card_is_active(credit_card:=user.credit_card):\n    if not (credit_card.type == \"Visa\" and online_payment_service_enabled(credit_card)):\n        raise OnlinePaymentServiceNotEnabledException(\"Visa Payment not possible.\")\n    process_payment(credit_card, payment_details)\nelse:\n    raise PaymentException(\"...\")\n\n\ndef is_solvent(user: User)-&gt;bool:\n    return user.state.value == \"solvent\"\n\ndef credit_card_is_active(credit_card: CreditCard) -&gt; bool:\n    return credit_card.get_expiration_dt &lt; datetime.datetime.now().date\n\ndef online_payment_service_enabled(credit_card: CreditCard)-&gt;bool:\n    return \"online_payment\" in user.credit_card.payment_services\n</code></pre>","tags":["clean code","comments"]},{"location":"blog/2023/08/01/5-reasons-why-code-comments-are-a-code-smell/#3-comments-that-describe-the-implementation","title":"3. Comments that describe the implementation","text":"<pre><code>def calculate_distance(x: Vector, y: Vector) -&gt; float:\n\"\"\"The distance is calculated by:\n            1. Calculate the difference between each components of the 2 vectors\n            2. Square the differences\n            3. Sum up \n            4. Take the squareroot\n    \"\"\"\n\n    square_sum = 0\n    for i in range(3):\n        difference = x[i] - y[i]\n        square = difference**2\n        square_sum += square\n\n    return sqrt(square_sum)\n</code></pre> <p>Why do we need this? The code described perfectly what is happening and there is not need to describe the implementation, also because the implementation can be changed in different ways (e.g. using a high-level libary like <code>numpy</code>).</p>","tags":["clean code","comments"]},{"location":"blog/2023/08/01/5-reasons-why-code-comments-are-a-code-smell/#4-comments-can-be-outdated","title":"4. Comments can be outdated","text":"<p>When code is changed or refactored, the test suite should ensure that the business logic and functionality of the program is still valid. However, comments are like dead code and are likely to be untouched by the programmer at hand. Thus, if the implementation changes, the comments are more likely to not describe the logic anymore. Even worse, these outdated ...</p>","tags":["clean code","comments"]},{"location":"blog/2023/08/01/5-reasons-why-code-comments-are-a-code-smell/#5-comments-can-be-missleading","title":"5. ... Comments can be missleading","text":"<p>Comments can be missleading, because they are not well written (\"hey, this is just a comment. No need to really be carefull writing these ...\") or are outdated (like stated above). </p> <p>Consider the following example: <pre><code># User is an adult:\nif user_age &gt;= 18:\n    # User is active\n    if user_is_active:\n        process_payment()\n    else:\n        raise UserNotActiveException()\nelse:\n    raise UserNotOldEnoughtException()\n</code></pre></p> <p>This is a pretty nested logic for such an easy business logic. Thus, we should invert the conditions to simplify the processing logic. So let us refactor this a bit <pre><code># User is an adult:\nif user_age &lt; 18:\n    raise UserNotOldEnoughtException()\n# User is active\nif not user_is_active:\n    raise UserNotActiveException()\nprocess_payment()\n</code></pre></p> <p>This code is obviously better, so the refactoring was a good idea. However, if you look closely, we forgot to adjust the code comments and now they are missleading.</p>","tags":["clean code","comments"]},{"location":"blog/2023/08/01/5-reasons-why-code-comments-are-a-code-smell/#6-outcommmented-code","title":"6. Outcommmented code","text":"<p>Since we all have a good version control system at hand today (#git), there is really no way of storing old or unfinished code in the production branch. Keep these <code>dead code</code> parts away from production, PERIOD.</p> <p></p>","tags":["clean code","comments"]},{"location":"blog/2023/08/01/5-reasons-why-code-comments-are-a-code-smell/#7-comments-hide-bad-naming","title":"7. Comments hide bad naming","text":"<p>Often, comments are used to describe a variable, because there was no effort to try to name the variable properly. Look at the following example:</p> <pre><code>import datetime\n\n\ndef check_token_validity(token_dt):\n    # We check that the token timestamp is only 1 hour old\n    return datetime.datetime.now().timestamp() - token_dt &lt; 3600\n</code></pre> <p>There are several things that we can improve on this code snippet:</p> <ol> <li>The name <code>token_dt</code> implies that we have a <code>datetime</code> object at hand. However, if we look closely, we see that it is a UTC Timestamp!</li> <li>Doing calculation in comparisons is generally a bad idea, because it makes the expression to complex to directly understand it</li> <li>What is the 3600 doing there? What is the unit and why was it chosen? We should make it clearer what this number means.</li> </ol> <p>Now, let us look at the refactored code: <pre><code>import datetime\n\nMAXIMUM_TOKEN_LIFETIME_IN_SECONDS = 60 * 60\n\ndef check_token_validity(token_timestamp: float) -&gt; bool:\n    current_timestamp = datetime.datetime.now().timestamp()\n    token_lifetime = current_timestamp - token_timestamp\n    return token_lifetime &lt; MAXIMUM_TOKEN_LIFETIME_IN_SECONDS\n</code></pre></p> <p>We have: 1. </p>","tags":["clean code","comments"]},{"location":"blog/2023/08/01/5-reasons-why-code-comments-are-a-code-smell/#good-code-comments","title":"Good Code Comments","text":"<p>After we have seen, that most common patterns of code comments are actually not a great idea, let us look at the exceptions:</p>","tags":["clean code","comments"]},{"location":"blog/2023/08/01/5-reasons-why-code-comments-are-a-code-smell/#1-code-documentation","title":"1. Code documentation","text":"<p>Code documentation for public APIs like docstrings can be a great idea, especially for libraries that are consumed by 3rd parties. These docstrings can then be picked up by documentation tools like Sphinx or MKDocs for an automated generation of nice API documentation.</p> <p>Note, that is in general not necessary to put a code comment on private functions that are not part of the public API, because they should in general not be used by external user and typically have a higher rate of change than the public APIs. Therefore, they are prone to outdated and missleading docstrings/comments as described above. Typically, for these functions you should invest in a good function name that describes properly what it does without the need of additional commentary. If this is not possible, this is often a sign of a function that does 2 or more things and should therefore be split up and refactored to reduce its complexity. </p>","tags":["clean code","comments"]},{"location":"blog/2023/08/01/5-reasons-why-code-comments-are-a-code-smell/#2-links-and-additional-information-about-business-logic","title":"2. Links and additional information about business logic","text":"<p>If a programm is written as a feature of a e.g. story, it may be a good idea to reference the story at the high-level code interface. Thus, when another programmer wants to know, why a functionality has been developed in the first place, he can go back to the story or additional user documentation.</p>","tags":["clean code","comments"]},{"location":"blog/2023/08/01/5-reasons-why-code-comments-are-a-code-smell/#3-additional-details-that-explain-the-reasoning-for-the-implementation-if-the-latter-is-not-obvious","title":"3. Additional details that explain the reasoning for the implementation if the latter is not obvious","text":"<p>There are code implementations or bugfixes that may not be obvious for a reader without the corresponding context. For these, there should be a reference to the corresponding issue or a comment describing the background information necessary to understand the maybe complex implementation. Think about something like this:</p> <pre><code>import time\n\nrun_id = start_run()\n\n# We have to wait a few seconds for the server to respond after the submission of a run. If we query for the run status\n# too early, we will get an error, because the internal database of the service has to catch up:\ntime.sleep(WAIT_TIME_IN_SECONDS)\n\nwhile True:\n    run_finished = poke_for_run(run_id)\n    if run_finished:\n        break\n</code></pre> <p>A programmer without detailed knowledge of the external system will need additional information about the intend of a chosen solution to understand the reasoning.</p>","tags":["clean code","comments"]},{"location":"blog/2023/08/01/5-reasons-why-code-comments-are-a-code-smell/#4-todo-comments","title":"4. TODO Comments","text":"<p>Sometimes, when working on code we find a part of the code that is not well written or might have a bad side effect in the future, but we do not have the time to directly work on the problem. Or we see that we can improve an implementation and make it way more efficient. Leaving a <code># TODO</code> comment can be a great way of finding those pieces in your code to later work on them.</p>","tags":["clean code","comments"]},{"location":"blog/2023/08/01/5-reasons-why-code-comments-are-a-code-smell/#5-warnings","title":"5. Warnings","text":"<p>Sometimes, there is a strange side effect or just something that a user has to know before he is calling the function at hand. Here, a warning comment may be appropriate.</p>","tags":["clean code","comments"]},{"location":"blog/2023/06/12/marp-markdown-presentation-ecosystem/","title":"Marp: Markdown Presentation Ecosystem","text":"","tags":["markdown","presentation"]},{"location":"blog/2023/06/12/marp-markdown-presentation-ecosystem/#what-is-marp","title":"What is Marp?","text":"<p>Quote</p> <p>Marp (also known as the Markdown Presentation Ecosystem) provides an intuitive experience for creating beautiful slide decks. You only have to focus on writing your story in a Markdown document.</p> <p>Here you find a few good references if you wish to get started using Marp:</p> <ul> <li>https://marp.app/</li> <li>https://marpit.marp.app/</li> <li>https://chris-ayers.com/2023/03/31/customizing-marp</li> <li>https://www.youtube.com/watch?v=EzQ-p41wNEE</li> </ul>","tags":["markdown","presentation"]},{"location":"blog/2023/06/12/marp-markdown-presentation-ecosystem/#how-to-build-a-cool-presentation","title":"How to build a cool presentation","text":"<p>Marp is a really powerful tool to build an HTML presentation using Markdown. It can be developed inside VS Code (see e.g. https://www.youtube.com/watch?v=EzQ-p41wNEE) and the HTML can be directly exported there.</p> <p>Let me give you an example presentation that shows how easy it is: </p> <p> Marp Example Presenation</p>","tags":["markdown","presentation"]},{"location":"blog/2023/06/12/marp-markdown-presentation-ecosystem/#example","title":"Example:","text":"<pre><code>---\npaginate: true\nmarp: true\ntheme: uncover\nclass: invert\nstyle: |\n  .small-red-text {\n    font-size: 0.75rem;\n    color: red;\n  }\n\n  h1 {\n    font-size: 60px;\n  }\n\n  h2 {\n    font-size: 50px;\n  } \n\n  h3 {\n    font-size: 40px;\n  }\n\n  h4 {\n    font-size: 30px;\n  }\n\n  h5,h6,p,li,code,table {\n    font-size: 25px;\n  }\nheadingDivider: 1\nmath: mathjax\n---\n\n\n# **Marp**\n\n\n![bg left:40% 80%](https://marp.app/assets/marp.svg)\n\nMarkdown Presentation Ecosystem\n\nhttps://marp.app/\n\n\n# How to write slides\n\nSplit pages by horizontal ruler (`---`). It's very simple! :satisfied:\n\n```markdown\n# Slide 1\n\nfoobar\n\n\n# Slide 2\n\nfoobar\n```\n\nAlternatively, set the **directive** [headingDivider](https://marpit.marp.app/directives?id=heading-divider). To automatically split at `h1` headers add to the document metadata:\n```md\nheadingDivider: 1\n```\n\n\n# Markdown !!!\n\n*Just* **write** `Markdown` ==here==!!!\n\n- Bullet 1\n- Bullet 2\n\n### Subtitle\n\n[Marp Link](https://marpit.marp.app/)\n\n**Emojies**: :joy: :wink: :smile: :rocket:\n\n\n# Markdown !!!\n#### Use automatic syntax highlighting\n```python\ndef foo(bar: str) -&gt; str:\n    return bar.strip()\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n\n# Markdown !!!\n\n### Tables\n\n| Syntax      | Description | Test Text     |\n| :---        |    :----:   |          ---: |\n| Header      | Title       | Here's this   |\n| Paragraph   | Text        | And more      |\n\n\n# Markdown !!!\n\n## Crazy Math\n&lt;!-- _footer: You have to add **math: mathjax** to the document metadata --&gt;\n$$ \nf(a) = \\frac{1}{2 \\pi i} \\oint_\\gamma \\frac{f(z)}{z-a} dz\n$$\n\n\n# Custom Backgrounds\n\n![bg](https://patrikhlobil.github.io/Blog/blogs/images/background.png)\n\nUse **Image Directives** (local or web-resource):\n\n```md\n![bg](https://patrikhlobil.github.io/Blog/blogs/images/background.png)\n```\n\n\n# Custom Backgrounds\n\n![bg](https://patrikhlobil.github.io/Blog/blogs/images/cats.jpg)\n\n![bg](https://patrikhlobil.github.io/Blog/blogs/images/sea.jpg)\n**Multiple Backgrounds!!!**\n\n```md\n![bg](https://patrikhlobil.github.io/Blog/blogs/images/cats.jpg)\n\n![bg](https://patrikhlobil.github.io/Blog/blogs/images/sea.jpg)\n```\n\n\n# Images\n\nPut Images to the left\n\n![bg left](https://patrikhlobil.github.io/Blog/blogs/images/cats.jpg)\n```md\n![bg left](https://patrikhlobil.github.io/Blog/blogs/images/cats.jpg)\n```\n\n# Images\n\n... or to the right\n\n![bg right](https://patrikhlobil.github.io/Blog/blogs/images/cats.jpg)\n```md\n![bg right](https://patrikhlobil.github.io/Blog/blogs/images/cats.jpg)\n```\n\n# Images\n\n... or adjust the width ...\n\n![bg left:25%](https://patrikhlobil.github.io/Blog/blogs/images/cats.jpg)\n```md\n![bg left:25%](https://patrikhlobil.github.io/Blog/blogs/images/cats.jpg)\n```\n\n\n# Images\n\n... or apply filters:\n\n![bg left sepia](https://patrikhlobil.github.io/Blog/blogs/images/sea.jpg)\n![bg right blur](https://patrikhlobil.github.io/Blog/blogs/images/sea.jpg)\n```md\n![bg left sepia](https://patrikhlobil.github.io/Blog/blogs/images/sea.jpg)\n![bg right blur](https://patrikhlobil.github.io/Blog/blogs/images/sea.jpg)\n```\n\n\n# GIFS !!!\n```\n![bg right](https://media0.giphy.com/media/SbtWGvMSmJIaV8faS8/200w.webp?\ncid=ecf05e47i5eyozrdt5aytc41m0o7ea6uxu6ck2088uxo4ojz&amp;ep=v1_gifs_search&amp;rid=200w.webp&amp;ct=g)\n```\n![bg](https://media0.giphy.com/media/SbtWGvMSmJIaV8faS8/200w.webp?cid=ecf05e47i5eyozrdt5aytc41m0o7ea6uxu6ck2088uxo4ojz&amp;ep=v1_gifs_search&amp;rid=200w.webp&amp;ct=g)\n\n\n# Mermaid Support\n\n&lt;!-- Add this anywhere in your Markdown file --&gt;\n&lt;script type=\"module\"&gt;\n  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';\n  mermaid.initialize({ startOnLoad: true });\n&lt;/script&gt;\n\n&lt;div class=\"mermaid\"&gt;\njourney\n    title My working day\n    section Go to work\n      Make tea: 5: Me\n      Go upstairs: 3: Me\n      Do work: 1: Me, Cat\n    section Go home\n      Go downstairs: 5: Me\n      Sit down: 3: Me\n&lt;/div&gt;\n\n# HTML Support\n\nAdd custom CSS to document metadata\n```\n---\nmarp: true\nstyle: |\n  .small-red-text {\n    font-size: 0.75rem;\n    color: red;\n  }\n---\n# Title\n&lt;div class=\"small-red-text\"&gt; Small Red Text&lt;/div&gt;\n```\n\n\n&lt;div class=\"small-red-text\"&gt; Small Red Text&lt;/div&gt;\n\n# HTML Support\n\n&lt;div id=\"myDiv\"&gt; &lt;/div&gt;\n&lt;script src='https://cdn.plot.ly/plotly-2.24.1.min.js'&gt;&lt;/script&gt;\n&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min.js'&gt;&lt;/script&gt;\n\n&lt;script&gt;\nd3.json('https://raw.githubusercontent.com/plotly/plotly.js/master/test/image/mocks/sankey_energy.json', function(fig){\n\nvar data = {\n  type: \"sankey\",\n  domain: {\n    x: [0,1],\n    y: [0,1]\n  },\n  orientation: \"h\",\n  valueformat: \".0f\",\n  valuesuffix: \"TWh\",\n  node: {\n    pad: 15,\n    thickness: 15,\n    line: {\n      color: \"black\",\n      width: 0.5\n    },\n   label: fig.data[0].node.label,\n   color: fig.data[0].node.color\n      },\n\n  link: {\n    source: fig.data[0].link.source,\n    target: fig.data[0].link.target,\n    value: fig.data[0].link.value,\n    label: fig.data[0].link.label\n  }\n}\n\nvar data = [data]\n\nvar layout = {\n  title: \"Energy forecast for 2050&lt;br&gt;Source: Department of Energy &amp; Climate Change, Tom Counsell via &lt;a href='https://bost.ocks.org/mike/sankey/'&gt;Mike Bostock&lt;/a&gt;\",\n  width: 1000,\n  height: 500,\n  font: {\n    size: 10\n  }\n}\n\nPlotly.newPlot('myDiv', data, layout)\n});\n\n&lt;/script&gt;\n\n\n# Fragmented lists\n\nUse `*` to get a fragmented list, where each item appears one after each other and `-` to directly display all items.\n\n### Non-Fragmented\n\n- non-frag 1\n- non-frag 2\n\n### Fragmented\n\n* frag 1\n* frag 2\n\n\n# Speaker Notes\n\nJust add **Speaker Notes** via:\n```html\n&lt;!-- \nSome notes here that might be useful.\n--&gt;\n```\n\n\n&lt;!-- \nSome notes here that might be useful.\n--&gt;\n\n# Directives\n\n&lt;!-- _backgroundColor: #180f61 --&gt;\n\nWith `Directives`, it is easy to modify the behaviour of **Marp** for a single slide or the whole presentation.\n\n#### Local Directives\n\nUsing local directives (prepend with `_`), one can modify a single page, e.g.\n```html\n&lt;!-- _backgroundColor: #180f61 --&gt;\n```\n\n#### Global Directives\n\nUsing global directives, one can modify the bejaviour of the slides from the current slide on\n```html\n&lt;!-- backgroundColor: aqua --&gt;\n```\n</code></pre>","tags":["markdown","presentation"]},{"location":"blog/2023/03/08/multiprocessing_and_fsspec/","title":"Multiprocessing and fsspec","text":"","tags":["python","multiprocessing","ffspec"]},{"location":"blog/2023/03/08/multiprocessing_and_fsspec/#caution-with-multiprocessing-and-ffspec-filesystems-in-python","title":"Caution with Multiprocessing and ffspec Filesystems in Python","text":"<p>Multiprocessing can be a great way of scaling Python applications beyond one CPU core, especially due slow speed compared to other lower-level languages like C or Java. However, one has to be very careful when passing Python class instance, especially if your are using a fsspec filesystem implementation. </p> <p>Note: At time of writing of this article the newest <code>fsspec</code> release was 2023.3.0.</p>","tags":["python","multiprocessing","ffspec"]},{"location":"blog/2023/03/08/multiprocessing_and_fsspec/#the-problem","title":"The Problem","text":"<p>Filesystem Spec (fsspec) is a Python package that provides a unified interface for filesystems. However, one has to be very carefull, when running tasks on an fsspec Filesystem in parallel. Let us consider the following code: <pre><code>import multiprocessing\nfrom fsspec import AbstractFileSystem\n\nmultiprocessing.set_start_method(\"fork\")\n\n\nclass MyClass:\n    def __init__(self):\n        self.store: dict[str, str] = {}\n\n\nclass MyFS(AbstractFileSystem):\n    def __init__(self):\n        self.store: dict[str, str] = {}\n\n\ndef return_store(my_class: MyClass) -&gt; str:\n    return f\"Class Name: {my_class.__class__.__name__: &lt;10} Store: {my_class.store}\"\n\n\n# Create class instance in parent process and add stuff to the internal store:\nmy_class = MyClass()\nmy_class.store[\"added\"] = \"content\"\n\nmy_fs = MyFS()\nmy_fs.store[\"added\"] = \"content\"\n\n\n# Run the function in a child process:\nwith multiprocessing.Pool(processes=2) as pool:\n    messages = pool.map(\n        return_store,\n        [my_class, my_fs],\n    )\n\n\n# Print out store content from child processes:\nfor message in messages:\n    print(message)\n</code></pre></p> <p>What we have here are two very simple classes <code>MyClass</code> and <code>MyFS</code> that just create an empty dictionary <code>store</code> attribute when instantiated. The key difference between these two classes is that <code>MyFS</code> is inherited from <code>fsspec.AbstractFileSystem</code>. We then create an instance of each of the two classes and add an entry to the <code>store</code> dictionary. Finally, we run a simple function in 2 child processes that just prints out the content of the <code>store</code> attribute. The resulting output of this script will be:</p> <pre><code>Class Name: MyClass    Store: {'added': 'content'}\nClass Name: MyFS       Store: {}\n</code></pre> <p>As can be seen, the <code>store</code> attribute of the <code>MyFS</code> class does not contain the added entry, whereas the same class without inheritance from the <code>AbstractFileSystem</code> does contain the non-empty dictionary. The reason behind these two different behaviours lies in the caching implementation of <code>fsspec</code>. The inheritance diagram looks like: <pre><code>---\ntitle: Class Diagram MyFS\n---\nclassDiagram\n    `fsspec.spec._Cached` &lt;|-- `fsspec.AbstractFileSystem`\n    `fsspec.AbstractFileSystem` &lt;|-- `MyFS`\n\n    class `fsspec.spec._Cached`{\n        __init__()\n        __call__()\n    }\n    class `fsspec.AbstractFileSystem`{\n        + ...\n    }</code></pre></p> <p>This line in the <code>_Cached</code> base is responsible for removing all existing attributes on the class instance when it will be forked for the child process: <pre><code>class _Cached(type):\n    ...\n    def __call__(cls, *args, **kwargs):\n        ...\nif os.getpid() != cls._pid:\n# In a child process, this line is called and will clear all existing attributes:\ncls._cache.clear()        \n...\n</code></pre></p>","tags":["python","multiprocessing","ffspec"]},{"location":"blog/2023/03/08/multiprocessing_and_fsspec/#possible-solutions","title":"Possible solutions","text":"<p>If you really want to parallelize applications with an <code>fsspec</code> Filesystem, there are at least 2 solutions.</p>","tags":["python","multiprocessing","ffspec"]},{"location":"blog/2023/03/08/multiprocessing_and_fsspec/#use-threading","title":"Use threading","text":"<p>If you run the code above with a <code>multiprocessing.pool.ThreadPool</code> instead of a <code>multiprocessing.Pool</code>, it just runs fine and does not show the observed caching behaviour.                                                              </p>","tags":["python","multiprocessing","ffspec"]},{"location":"blog/2023/03/08/multiprocessing_and_fsspec/#instantiate-the-filesystem-inside-the-processes-itself","title":"Instantiate the filesystem inside the processes itself","text":"<p>Instead of passing the instatiated filesystem directly into the subprocess, it is better to use an Adapter that stores the information about the filesystem and can create an instance of it. Let us consider the following example: <pre><code>import multiprocessing\nfrom fsspec.implementations.github import GithubFileSystem\n\nmultiprocessing.set_start_method(\"fork\")\n\n\nclass GithubFileSystemAdapter:\n    def __init__(self, org: str, repo: str):\n        self.org = org\n        self.repo = repo\n\n    def get_github_fs(self) -&gt; GithubFileSystem:\n        return GithubFileSystem(org=self.org, repo=self.repo)\n\n\ndef list_root_directory_of_repository(\n    github_fs_adapter: GithubFileSystemAdapter,\n) -&gt; list[str]:\n    # Create fsspec filesystem inside child process:\n    github_fs = github_fs_adapter.get_github_fs()\n    return github_fs.ls(\"/\")\n\n\n# Create GithubFileSystemAdapter for 'https://github.com/python/cpython':\ngithub_fs_adapter = GithubFileSystemAdapter(org=\"python\", repo=\"cpython\")\n\n\n# Run the function in a child process:\nwith multiprocessing.Pool(processes=1) as pool:\n    messages = pool.map(\n        list_root_directory_of_repository,\n        [github_fs_adapter],\n    )\n\n# Print out store content from child process:\nfor message in messages:\n    print(message)\n</code></pre></p> <p>As can be seen, we define a container class <code>GithubFileSystemAdapter</code> that simply stores the data and implements a method for creating the filesystem. This container class is then passed to the child process, within which the <code>fsspec</code> filessystem object is created. It can then be safely used without the problems seen in the previous example.</p>","tags":["python","multiprocessing","ffspec"]}]}